{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joakim\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\joakim\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel, AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer, DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joakim\\AppData\\Local\\Temp\\ipykernel_18544\\3707943535.py:3: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(dataSetPath, sep=\"␞\", nrows=200000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n",
      "199493\n",
      "Index(['id', 'url', 'headline', 'text', 'subjects', 'datePublished'], dtype='object')\n",
      "text        object\n",
      "subjects    object\n",
      "dtype: object\n",
      "11\n",
      "['urheilu', 'ulkomaat', 'kotimaa', 'kulttuuri', 'talous', 'luonto', 'politiikka', 'sää', 'kolumnit', 'tiede', 'oppiminen']\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "dataSetPath = \"2011-2018-3.csv\"\n",
    "df = pd.read_csv(dataSetPath, sep=\"␞\", nrows=200000)\n",
    "# df = pd.read_csv(dataSetPath, sep=\"␞\", nrows=10000)\n",
    "\n",
    "\n",
    "print(df['text'].isna().sum())\n",
    "df = df.dropna(subset=['text'])\n",
    "print(len(df))\n",
    "\n",
    "print(df.columns)\n",
    "df = df.drop(['id', 'url', 'headline', 'datePublished'], axis=1) \n",
    "\n",
    "def multiLabelToSingular(labelArray):\n",
    "    cleaned_text = labelArray.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    return cleaned_text.split(\",\")[0]\n",
    "\n",
    "print(df.dtypes)\n",
    "df['subjects'] = df['subjects'].apply(multiLabelToSingular)\n",
    "print(df['subjects'].nunique())\n",
    "print(df['subjects'].unique().tolist())\n",
    "\n",
    "df.rename({'subjects': 'labels'}, axis=\"columns\", inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kolumnit        218\n",
      "kotimaa       54337\n",
      "kulttuuri      9317\n",
      "luonto         5204\n",
      "oppiminen       116\n",
      "politiikka     6871\n",
      "sää            2790\n",
      "talous        17154\n",
      "tiede          1081\n",
      "ulkomaat      32396\n",
      "urheilu       70009\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# paperin luokat:\n",
    "\n",
    "# urheilu \"18-220090\",\n",
    "# talous \"18-204933\",\n",
    "# politiikka \"18-220306\",\n",
    "# kulttuuri \"18-208149\",\n",
    "# luonto \"18-215452\",\n",
    "# 18-218480 - Onnettomuudet\n",
    "# 18-209306 - Terveys\n",
    "# 18-217206 - Rikokset\n",
    "# 18-91232 - Liikenne ja kuljetus\n",
    "# 18-35286 - Koulutus ja kasvatus\n",
    "\n",
    "print(df['labels'].value_counts().sort_index(ascending=True))\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.labels)\n",
    "df['labels'] = le.transform(df.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825\n",
      "0     75\n",
      "1     75\n",
      "2     75\n",
      "3     75\n",
      "4     75\n",
      "5     75\n",
      "6     75\n",
      "7     75\n",
      "8     75\n",
      "9     75\n",
      "10    75\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_sample = 75\n",
    "df2 = df.groupby('labels').head(n_sample)\n",
    "df2 = df.groupby('labels').apply(lambda x: x.sample(n=n_sample))\n",
    "\n",
    "print(len(df2))\n",
    "print(df2['labels'].value_counts().sort_index(ascending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 825\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df2, preserve_index=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 594\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 66\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 165\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_valid_split = train_test_split['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_valid_split['train'],\n",
    "    'validation': train_valid_split['test'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "unique_values = set(datasets[\"train\"][\"labels\"])\n",
    "print(unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokenized = tokenizer(example[\"text\"], truncation=True)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bcda5ae22b484dbab4b298a1df0826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/594 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fb1379cd154a62b5b8d16dca4829db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ac7010f0324ef4b8d93843c37de78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/165 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 594\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 66\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 165\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][0][\"token_type_ids\"])\n",
    "print(tokenized_datasets[\"train\"][1][\"token_type_ids\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",                    \n",
    "    evaluation_strategy=\"steps\",        \n",
    "    eval_steps=50,                      \n",
    "    logging_dir=\"./logs\",                \n",
    "    logging_steps=50,                   \n",
    "    save_steps=500,                      \n",
    "    per_device_train_batch_size=8,       \n",
    "    per_device_eval_batch_size=16,       \n",
    "    num_train_epochs=5,                  \n",
    "    save_total_limit=2,                  \n",
    "    load_best_model_at_end=True,         \n",
    "    metric_for_best_model=\"f1\",          \n",
    "    greater_is_better=True,              \n",
    "    report_to=\"tensorboard\",            \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\", num_labels=11, device_map = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joakim\\anaconda3\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4d128a89ba48e689312a33a954e768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678f2cdebff54dc688348a4581170e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1289596557617188, 'eval_accuracy': 0.6666666666666666, 'eval_f1': 0.6296296296296297, 'eval_runtime': 23.0231, 'eval_samples_per_second': 2.867, 'eval_steps_per_second': 0.217, 'epoch': 0.67}\n",
      "{'loss': 1.3383, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bd2f09f93c490e9b74ac5551d9bdb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7367255687713623, 'eval_accuracy': 0.7575757575757576, 'eval_f1': 0.7512972865914043, 'eval_runtime': 19.2805, 'eval_samples_per_second': 3.423, 'eval_steps_per_second': 0.259, 'epoch': 1.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf6eeab57154812b6b5441c2faff7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7671732306480408, 'eval_accuracy': 0.7727272727272727, 'eval_f1': 0.7665613352779128, 'eval_runtime': 19.3433, 'eval_samples_per_second': 3.412, 'eval_steps_per_second': 0.258, 'epoch': 2.0}\n",
      "{'loss': 0.4381, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e375054d2bc7491caec67418baa45133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6833579540252686, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8274799818917467, 'eval_runtime': 19.1539, 'eval_samples_per_second': 3.446, 'eval_steps_per_second': 0.261, 'epoch': 2.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a227ad56b6bc41eb94954099afb0a74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9282227754592896, 'eval_accuracy': 0.8181818181818182, 'eval_f1': 0.818374925727867, 'eval_runtime': 19.3877, 'eval_samples_per_second': 3.404, 'eval_steps_per_second': 0.258, 'epoch': 3.33}\n",
      "{'loss': 0.122, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3835abd4e14f7ca634a973d33bebfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8517866730690002, 'eval_accuracy': 0.803030303030303, 'eval_f1': 0.8050872874402287, 'eval_runtime': 19.1827, 'eval_samples_per_second': 3.441, 'eval_steps_per_second': 0.261, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78cab98897b47c780dcd0a60969e14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8831983208656311, 'eval_accuracy': 0.803030303030303, 'eval_f1': 0.803378810731752, 'eval_runtime': 19.4261, 'eval_samples_per_second': 3.397, 'eval_steps_per_second': 0.257, 'epoch': 4.67}\n",
      "{'train_runtime': 1108.443, 'train_samples_per_second': 2.679, 'train_steps_per_second': 0.338, 'train_loss': 0.5140714613596599, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.5140714613596599, metrics={'train_runtime': 1108.443, 'train_samples_per_second': 2.679, 'train_steps_per_second': 0.338, 'train_loss': 0.5140714613596599, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69312de60c84d378973fbb0edb29625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 11) (165,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  8  9  8  4  7  9  5  3  9 10 10  0  5  0  3  7  5  1  6  3  7  9  5\n",
      "  2  1  6  6  9  5  7  7 10  7  8  8  9  8  8 10  8  2  2  0  1  8  9 10\n",
      "  8  0  1  6  0  4 10  2  7  5  2  2  1 10 10  3  6  3  1  3  8 10  8  3\n",
      "  9  6  9 10  0  2  9 10  3  0  1  2 10  6  5 10  9  1  1  5  7  3 10  2\n",
      "  2  3  0  3  1  8  2  4  2  8  8  8  8  7  1  6  1  3  1  2  4 10 10 10\n",
      "  9  8  0  6  6  1  1  1  2  9  1  7  2 10 10 10  2  0  1  2  7  3  4  3\n",
      "  8  0 10  4  5  1 10  0  3  3  2  1  3  4  2  9  0 10  2  8  1]\n",
      "[ 9  8  9  8  0  7  9  5  3  9 10 10  0  5  0  3  7  5  1  6  1  7  9  5\n",
      "  2  5  0  6  7  5  7  1 10  7  8  8  9  8  8 10  8  2  2  0  5  8  9 10\n",
      "  8  0  5  6  0  4 10  0  1  5  2  2  5 10 10  3  6  3  7  5  3 10  8  3\n",
      "  9  6  9 10  0  2  5 10  3  0  4  2 10  9  5 10  1  5  2  5  7  1 10  9\n",
      "  2  6  0  3  7  8  2  4  2  4  8  1  8  7  1  6  5  3  7  2  8 10  1 10\n",
      "  6  8  0  6  6  1  8  1  2  8  1  7  2 10 10 10  2  3  1  2  5  3  8  3\n",
      "  8  0 10  4  5  0 10  0  3  3  2  1  3  4  7  9  0 10  2  9  1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "print(preds)\n",
    "print(predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7575757575757576\n",
      "F1 Score: 0.7575757575757576\n"
     ]
    }
   ],
   "source": [
    "# print(predictions.label_ids)\n",
    "\n",
    "accuracy = accuracy_score(tokenized_datasets[\"test\"][\"labels\"], preds)\n",
    "\n",
    "f1 = f1_score(tokenized_datasets[\"test\"][\"labels\"], preds, average=\"micro\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eba544abd604bfb9570173613dec158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0131111145019531,\n",
       " 'eval_accuracy': 0.7575757575757576,\n",
       " 'eval_f1': 0.7599448157878285,\n",
       " 'eval_runtime': 46.5168,\n",
       " 'eval_samples_per_second': 3.547,\n",
       " 'eval_steps_per_second': 0.236,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
