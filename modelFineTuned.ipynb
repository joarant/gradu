{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joakim\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\joakim\\anaconda3\\lib\\site-packages\\transformers\\utils\\generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertModel, AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer, DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joakim\\AppData\\Local\\Temp\\ipykernel_13308\\3043923669.py:3: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(dataSetPath, sep=\"␞\", nrows=200000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612 invalid data\n",
      "199388 valid data\n",
      "Index(['id', 'url', 'headline', 'text', 'subjects', 'datePublished'], dtype='object')\n",
      "10 categories\n",
      "['urheilu', 'kulttuuri', 'talous', 'luonto', 'Onnettomuudet', 'politiikka', 'Rikokset', 'Liikenne_ja_kuljetus', 'Koulutus_ja_kasvatus', 'Terveys']\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "dataSetPath = \"yle_data/2011-2018-SubjectsInPaper-1.csv\"\n",
    "df = pd.read_csv(dataSetPath, sep=\"␞\", nrows=200000)\n",
    "\n",
    "\n",
    "print(df['text'].isna().sum(), \"invalid data\")\n",
    "df = df.dropna(subset=['text'])\n",
    "print(len(df), \"valid data\")\n",
    "\n",
    "print(df.columns)\n",
    "df = df.drop(['id', 'url', 'headline', 'datePublished'], axis=1) \n",
    "\n",
    "\n",
    "def multiLabelToSingular(labelArray):\n",
    "    cleaned_text = labelArray.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    return cleaned_text.split(\",\")[0]\n",
    "\n",
    "\n",
    "# print(df.dtypes)\n",
    "df['subjects'] = df['subjects'].apply(multiLabelToSingular)\n",
    "\n",
    "print(df['subjects'].nunique(), \"categories\")\n",
    "print(df['subjects'].unique().tolist())\n",
    "\n",
    "df.rename({'subjects': 'labels'}, axis=\"columns\", inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koulutus_ja_kasvatus    10753\n",
      "Liikenne_ja_kuljetus    13554\n",
      "Onnettomuudet           10717\n",
      "Rikokset                 8166\n",
      "Terveys                  8600\n",
      "kulttuuri               13794\n",
      "luonto                  10994\n",
      "politiikka              12961\n",
      "talous                  24427\n",
      "urheilu                 85422\n",
      "Name: labels, dtype: int64\n",
      "0    10753\n",
      "1    13554\n",
      "2    10717\n",
      "3     8166\n",
      "4     8600\n",
      "5    13794\n",
      "6    10994\n",
      "7    12961\n",
      "8    24427\n",
      "9    85422\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# paperin luokat:\n",
    "\n",
    "# urheilu \"18-220090\",\n",
    "# talous \"18-204933\",\n",
    "# politiikka \"18-220306\",\n",
    "# kulttuuri \"18-208149\",\n",
    "# luonto \"18-215452\",\n",
    "# 18-218480 - Onnettomuudet\n",
    "# 18-209306 - Terveys\n",
    "# 18-217206 - Rikokset\n",
    "# 18-91232 - Liikenne ja kuljetus\n",
    "# 18-35286 - Koulutus ja kasvatus\n",
    "\n",
    "label_mapping = {\n",
    "    \"Koulutus_ja_kasvatus\": 0,\n",
    "    \"Liikenne_ja_kuljetus\": 1,\n",
    "    \"Onnettomuudet\": 2,\n",
    "    \"Rikokset\": 3,\n",
    "    \"Terveys\": 4,\n",
    "    \"kulttuuri\": 5,\n",
    "    \"luonto\": 6,\n",
    "    \"politiikka\": 7,\n",
    "    \"talous\": 8,\n",
    "    \"urheilu\": 9\n",
    "}\n",
    "\n",
    "print(df['labels'].value_counts().sort_index(ascending=True))\n",
    "df[\"labels\"] = df[\"labels\"].map(label_mapping)\n",
    "print(df['labels'].value_counts().sort_index(ascending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 documents\n",
      "0    75\n",
      "1    75\n",
      "2    75\n",
      "3    75\n",
      "4    75\n",
      "5    75\n",
      "6    75\n",
      "7    75\n",
      "8    75\n",
      "9    75\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "n_sample = 75\n",
    "df_balanced = df.groupby('labels').head(n_sample)\n",
    "df_balanced = df.groupby('labels').apply(lambda x: x.sample(n=n_sample))\n",
    "\n",
    "print(len(df_balanced), \"documents\")\n",
    "print(df_balanced['labels'].value_counts().sort_index(ascending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels'],\n",
       "    num_rows: 750\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_balanced, preserve_index=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 540\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 60\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_valid_split = train_test_split['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_valid_split['train'],\n",
    "    'validation': train_valid_split['test'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "unique_values = set(datasets[\"train\"][\"labels\"])\n",
    "print(unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokenized = tokenizer(example[\"text\"], truncation=True)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34df7b9af0494392a9ff4dec168d1271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b9001f735347dead57cb64adcc391a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed296f5035945e692391d53ce341676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 540\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./logs\",                    \n",
    "    evaluation_strategy=\"steps\",        \n",
    "    eval_steps=50,                      \n",
    "    logging_dir=\"./logs\",                \n",
    "    logging_steps=50,                   \n",
    "    save_steps=500,                      \n",
    "    per_device_train_batch_size=8,       \n",
    "    per_device_eval_batch_size=16,       \n",
    "    num_train_epochs=5,                  \n",
    "    save_total_limit=2,                  \n",
    "    load_best_model_at_end=True,         \n",
    "    metric_for_best_model=\"f1\",          \n",
    "    greater_is_better=True,              \n",
    "    report_to=\"tensorboard\",            \n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at TurkuNLP/bert-base-finnish-cased-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\joakim\\anaconda3\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\", num_labels=11, device_map = 'cuda')\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e0225787ad4fcd8ead5a9e35c29c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/340 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.824, 'learning_rate': 4.2647058823529415e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfeeed9fae541729d5e3fc847347181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.110843300819397, 'eval_accuracy': 0.75, 'eval_f1': 0.7523455710955711, 'eval_runtime': 7.8434, 'eval_samples_per_second': 7.65, 'eval_steps_per_second': 0.51, 'epoch': 0.74}\n",
      "{'loss': 0.6757, 'learning_rate': 3.529411764705883e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ac4243b4914bb1937ae43c65afb792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7637470364570618, 'eval_accuracy': 0.7833333333333333, 'eval_f1': 0.7826752822341058, 'eval_runtime': 7.3849, 'eval_samples_per_second': 8.125, 'eval_steps_per_second': 0.542, 'epoch': 1.47}\n",
      "{'loss': 0.513, 'learning_rate': 2.7941176470588236e-05, 'epoch': 2.21}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce866d39ed994bdc83aee773e36d60cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5713123679161072, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8316857298474946, 'eval_runtime': 7.5783, 'eval_samples_per_second': 7.917, 'eval_steps_per_second': 0.528, 'epoch': 2.21}\n",
      "{'loss': 0.2432, 'learning_rate': 2.058823529411765e-05, 'epoch': 2.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f8af1a308a47168007a40bd3488dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5637201070785522, 'eval_accuracy': 0.8166666666666667, 'eval_f1': 0.8192527652086475, 'eval_runtime': 7.9604, 'eval_samples_per_second': 7.537, 'eval_steps_per_second': 0.502, 'epoch': 2.94}\n",
      "{'loss': 0.0861, 'learning_rate': 1.323529411764706e-05, 'epoch': 3.68}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d4078415c54067a6f5810ac27c5491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6704396605491638, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8334780578898225, 'eval_runtime': 7.2733, 'eval_samples_per_second': 8.249, 'eval_steps_per_second': 0.55, 'epoch': 3.68}\n",
      "{'loss': 0.044, 'learning_rate': 5.882352941176471e-06, 'epoch': 4.41}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea01aa4ad17443a87f6e5b7903c35bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5652924180030823, 'eval_accuracy': 0.8666666666666667, 'eval_f1': 0.8622954822954821, 'eval_runtime': 7.28, 'eval_samples_per_second': 8.242, 'eval_steps_per_second': 0.549, 'epoch': 4.41}\n",
      "{'train_runtime': 877.174, 'train_samples_per_second': 3.078, 'train_steps_per_second': 0.388, 'train_loss': 0.5043766828144298, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=0.5043766828144298, metrics={'train_runtime': 877.174, 'train_samples_per_second': 3.078, 'train_steps_per_second': 0.388, 'train_loss': 0.5043766828144298, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3725a9729454bbb8ce8d59e0e3088c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 11) (150,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset=tokenized_datasets[\"test\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 6 7 0 2 4 9 2 6 0 6 8 9 1 6 1 2 2 8 5 3 3 2 7 3 2 1 8 6 6 5 1 1 7 0 1 6\n",
      " 5 0 6 4 4 2 5 7 8 5 7 4 1 1 2 3 0 0 3 2 3 4 8 1 3 9 9 5 1 9 3 3 8 1 1 9 4\n",
      " 7 6 4 9 5 7 3 0 2 9 7 0 0 9 0 2 6 5 1 9 8 0 6 9 2 4 8 7 4 8 1 7 5 3 1 7 4\n",
      " 8 7 4 5 1 1 2 9 7 6 2 0 0 6 2 5 8 7 6 3 1 7 0 3 0 8 2 3 6 1 6 0 7 5 9 8 7\n",
      " 3 4]\n",
      "[0 6 7 0 2 4 9 2 6 0 6 8 9 1 6 9 2 2 8 5 3 3 2 7 3 2 2 8 6 6 5 8 1 8 0 1 4\n",
      " 5 0 6 4 4 2 5 7 8 5 8 4 1 1 2 3 0 3 3 2 3 4 8 1 3 9 9 5 1 9 3 3 8 2 1 9 4\n",
      " 8 6 4 9 5 8 5 0 5 9 7 0 0 9 0 2 6 5 1 9 8 0 6 9 1 4 8 8 4 8 6 7 5 3 1 8 4\n",
      " 8 7 4 0 2 1 2 9 7 6 2 0 5 6 1 5 8 7 6 3 1 7 0 3 0 8 2 3 6 1 6 0 7 5 9 8 7\n",
      " 3 4]\n",
      "accuracy= 0.8666666666666667 f1= 0.8681607400587794\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "print(preds)\n",
    "print(predictions.label_ids)\n",
    "\n",
    "accuracy = accuracy_score(predictions.label_ids, preds)\n",
    "f1 = f1_score(predictions.label_ids, preds, average='weighted')\n",
    "print(\"accuracy=\" , accuracy, \"f1=\" , f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a165dc64423247258752c4bb3e9a57e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5735817551612854,\n",
       " 'eval_accuracy': 0.8666666666666667,\n",
       " 'eval_f1': 0.8681607400587794,\n",
       " 'eval_runtime': 17.8206,\n",
       " 'eval_samples_per_second': 8.417,\n",
       " 'eval_steps_per_second': 0.561,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_modernized</th>\n",
       "      <th>text_better_ocr</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kuulumiset ulkomailta ovat olleet hyvin vähäis...</td>\n",
       "      <td>Kuulumiset ulkomailta owat olleet hywin wähäi-...</td>\n",
       "      <td>Rikokset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amerikasta kerrotaan, että New Yorkiin, Amerik...</td>\n",
       "      <td>Amerikasta kerrotaan, että NewYork'iin Amerika...</td>\n",
       "      <td>Liikenne_ja_kuljetus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kurmijärveltä kirjoitetaan meille joulukuun lo...</td>\n",
       "      <td>Kurmijärweltä kirjoitetaan meille lopulla wii-...</td>\n",
       "      <td>Rikokset</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text_modernized  \\\n",
       "0  Kuulumiset ulkomailta ovat olleet hyvin vähäis...   \n",
       "1  Amerikasta kerrotaan, että New Yorkiin, Amerik...   \n",
       "2  Kurmijärveltä kirjoitetaan meille joulukuun lo...   \n",
       "\n",
       "                                     text_better_ocr                labels  \n",
       "0  Kuulumiset ulkomailta owat olleet hywin wähäi-...              Rikokset  \n",
       "1  Amerikasta kerrotaan, että NewYork'iin Amerika...  Liikenne_ja_kuljetus  \n",
       "2  Kurmijärweltä kirjoitetaan meille lopulla wii-...              Rikokset  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "test_corpus_data = \"test_corpus_data.csv\" # 63 dokumenttia\n",
    "df_corpus = pd.read_csv(test_corpus_data, sep=\"|\")\n",
    "\n",
    "df_corpus.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     3\n",
      "1     1\n",
      "2     3\n",
      "3     7\n",
      "4     7\n",
      "     ..\n",
      "58    7\n",
      "59    7\n",
      "60    7\n",
      "61    6\n",
      "62    8\n",
      "Name: labels, Length: 63, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_corpus['labels'] = df_corpus.labels.map(label_mapping)\n",
    "print(df_corpus.labels)\n",
    "\n",
    "df_corpus_modernized = df_corpus.copy()\n",
    "\n",
    "df_corpus.rename({'text_better_ocr': 'text'}, axis=\"columns\", inplace=True) # parempi ocr-teksti on kopioitu kansalliskirjaston sivuilta. alkuperäisen datasetin ocr-data on heikkolaatuista \n",
    "df_corpus_modernized.rename({'text_modernized': 'text'}, axis=\"columns\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = Dataset.from_pandas(df_corpus, preserve_index=False)\n",
    "df_corpus_modernized = Dataset.from_pandas(df_corpus_modernized, preserve_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84694d33dd964399b7d9e79f62a19691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1391aa8a701a4e6fa80755f75d9c09b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'text_better_ocr', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 63\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = df_corpus.map(tokenize_function, batched=True)\n",
    "tokenized_corpus\n",
    "\n",
    "tokenized_corpus_modern = df_corpus_modernized.map(tokenize_function, batched=True)\n",
    "tokenized_corpus_modern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8906aeab23fe4334a2c9422914566dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 7 6 7 7 7 7 7 8 1 6 7 8 7 7 7 8 0 5 8 8 5 1 8 6 7 0 5 7 7 7 7 8 8 0 0 7\n",
      " 7 8 8 7 8 6 7 8 8 8 7 7 5 7 5 5 7 6 7 8 7 7 7 7 7 6]\n",
      "[3 1 3 7 7 7 7 7 8 1 1 5 1 7 7 7 8 7 5 8 8 5 1 8 3 3 0 7 7 7 7 7 8 7 0 7 5\n",
      " 7 8 8 5 8 6 5 8 8 8 7 8 5 1 5 5 1 6 1 8 7 7 7 7 6 8]\n",
      "accuracy= 0.6666666666666666 f1= 0.6381841625539105\n"
     ]
    }
   ],
   "source": [
    "predictions_corpus = trainer.predict(test_dataset=tokenized_corpus)\n",
    "preds_corpus = np.argmax(predictions_corpus.predictions, axis=-1)\n",
    "print(preds_corpus)\n",
    "print(predictions_corpus.label_ids)\n",
    "\n",
    "accuracy = accuracy_score(predictions_corpus.label_ids, preds_corpus)\n",
    "f1 = f1_score(predictions_corpus.label_ids, preds_corpus, average='weighted')\n",
    "print(\"accuracy=\" , accuracy, \"f1=\" , f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9c317aa644484f932fbcf8aa363674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 6 7 7 7 7 8 8 1 7 7 8 3 7 7 8 0 5 8 8 5 1 3 3 3 0 5 7 7 7 7 8 8 0 0 7\n",
      " 7 8 8 7 8 2 7 8 8 8 7 7 5 4 5 5 7 6 1 8 7 7 7 7 7 6]\n",
      "[3 1 3 7 7 7 7 7 8 1 1 5 1 7 7 7 8 7 5 8 8 5 1 8 3 3 0 7 7 7 7 7 8 7 0 7 5\n",
      " 7 8 8 5 8 6 5 8 8 8 7 8 5 1 5 5 1 6 1 8 7 7 7 7 6 8]\n",
      "accuracy= 0.6507936507936508 f1= 0.6613965744400527\n"
     ]
    }
   ],
   "source": [
    "predictions_corpus = trainer.predict(test_dataset=tokenized_corpus_modern)\n",
    "preds_corpus = np.argmax(predictions_corpus.predictions, axis=-1)\n",
    "print(preds_corpus)\n",
    "print(predictions_corpus.label_ids)\n",
    "\n",
    "accuracy = accuracy_score(predictions_corpus.label_ids, preds_corpus)\n",
    "f1 = f1_score(predictions_corpus.label_ids, preds_corpus, average='weighted')\n",
    "print(\"accuracy=\" , accuracy, \"f1=\" , f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t_model\\\\tokenizer_config.json',\n",
       " './t_model\\\\special_tokens_map.json',\n",
       " './t_model\\\\vocab.txt',\n",
       " './t_model\\\\added_tokens.json',\n",
       " './t_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"./t_model\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
